{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA Assignment 20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD51dzvickcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import six\n",
        "import sys\n",
        "import tarfile\n",
        "import requests\n",
        "from logging import getLogger, StreamHandler, DEBUG\n",
        "logger = getLogger(__name__)\n",
        "handler = StreamHandler()\n",
        "logger.addHandler(handler)\n",
        "logger.propagate = False\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except:\n",
        "    import pickle\n",
        "\n",
        "try:\n",
        "    from itertools import zip_longest\n",
        "except:\n",
        "    from itertools import izip_longest as zip_longest\n",
        "\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "\n",
        "class Spinner:\n",
        "    busy = False\n",
        "    delay = 0.1\n",
        "\n",
        "    @staticmethod\n",
        "    def spinning_cursor():\n",
        "        while 1:\n",
        "            for cursor in '|/-\\\\': yield cursor\n",
        "\n",
        "    def __init__(self, delay=None, prefix=None):\n",
        "        self.prefix = prefix\n",
        "        self.prefix_len = len(prefix)\n",
        "        self.spinner_generator = self.spinning_cursor()\n",
        "        if delay and float(delay): self.delay = delay\n",
        "\n",
        "    def spinner_task(self):\n",
        "        while self.busy:\n",
        "            sys.stdout.write(self.prefix)\n",
        "            sys.stdout.write(next(self.spinner_generator))\n",
        "            sys.stdout.flush()\n",
        "            time.sleep(self.delay)\n",
        "            sys.stdout.write('\\r')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    def start(self):\n",
        "        self.busy = True\n",
        "        threading.Thread(target=self.spinner_task).start()\n",
        "\n",
        "    def stop(self):\n",
        "        self.busy = False\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(' ' * (self.prefix_len + 1))\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(self.delay)\n",
        "\n",
        "\n",
        "CIFAR10_URL  = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "CIFAR100_URL = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
        "CIFAR10_TAR_FILENAME  = 'cifar-10-python.tar.gz'\n",
        "CIFAR100_TAR_FILENAME = 'cifar-100-python.tar.gz'\n",
        "CIFAR10_TAR_MD5  = 'c58f30108f718f92721af3b95e74349a'\n",
        "CIFAR100_TAR_MD5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
        "\n",
        "CIFAR10_TRAIN_DATA_NAMES = [\n",
        "    'cifar-10-batches-py/data_batch_1',\n",
        "    'cifar-10-batches-py/data_batch_2',\n",
        "    'cifar-10-batches-py/data_batch_3',\n",
        "    'cifar-10-batches-py/data_batch_4',\n",
        "    'cifar-10-batches-py/data_batch_5'\n",
        "]\n",
        "CIFAR10_TEST_DATA_NAMES   = ['cifar-10-batches-py/test_batch']\n",
        "CIFAR100_TRAIN_DATA_NAMES = ['cifar-100-python/train']\n",
        "CIFAR100_TEST_DATA_NAMES  = ['cifar-100-python/test']\n",
        "\n",
        "CIFAR10_LABELS_LIST = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "]\n",
        "CIFAR100_LABELS_LIST = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "CIFAR100_SUPERCLASS_LABELS_LIST = [\n",
        "    'aquatic_mammals', 'fish', 'flowers', 'food_containers',\n",
        "    'fruit_and_vegetables', 'household_electrical_devices',\n",
        "    'household_furniture', 'insects', 'large_carnivores',\n",
        "    'large_man-made_outdoor_things', 'large_natural_outdoor_scenes',\n",
        "    'large_omnivores_and_herbivores', 'medium_mammals',\n",
        "    'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals',\n",
        "    'trees', 'vehicles_1', 'vehicles_2'\n",
        "]\n",
        "CIFAR100_CLASSES_LABELS_LIST = [\n",
        "    ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
        "    ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
        "    ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
        "    ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
        "    ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
        "    ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
        "    ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
        "    ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
        "    ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
        "    ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
        "    ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
        "    ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
        "    ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
        "    ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
        "    ['baby', 'boy', 'girl', 'man', 'woman'],\n",
        "    ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
        "    ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
        "    ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
        "    ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
        "    ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
        "]\n",
        "\n",
        "\n",
        "def unpickle(dump):\n",
        "    if six.PY2:\n",
        "        data = pickle.loads(dump.read())\n",
        "    elif six.PY3:\n",
        "        data = pickle.loads(dump.read(), encoding='latin1')\n",
        "    return data\n",
        "\n",
        "\n",
        "def check_output_path(output):\n",
        "    outputdir = Path(output)\n",
        "    if outputdir.exists():\n",
        "        logger.error(\"output dir `{}` already exists. Please specify a different output path\".format(output))\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "# Reference: https://stackoverflow.com/questions/37573483/progress-bar-while-download-file-over-http-with-requests/37573701\n",
        "def download_with_progress(url, filename):\n",
        "    logger.warning(\"Downloading {}\".format(filename))\n",
        "    r = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    block_size = 1024\n",
        "    wrote = 0\n",
        "    with open(filename, 'wb') as f:\n",
        "        for data in tqdm(r.iter_content(block_size), total=math.ceil(total_size//block_size) , unit='KB', unit_scale=True):\n",
        "            wrote = wrote  + len(data)\n",
        "            f.write(data)\n",
        "    if total_size != 0 and wrote != total_size:\n",
        "        logger.error(\"ERROR, something went wrong\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def download_cifar(dataset):\n",
        "    if dataset == 'cifar10':\n",
        "        download_with_progress(CIFAR10_URL, CIFAR10_TAR_FILENAME)\n",
        "    elif dataset in ['cifar100', 'cifar100superclass']:\n",
        "        download_with_progress(CIFAR100_URL, CIFAR100_TAR_FILENAME)\n",
        "\n",
        "\n",
        "def check_cifar(dataset):\n",
        "    if dataset == 'cifar10':\n",
        "        cifar = Path(CIFAR10_TAR_FILENAME)\n",
        "        md5sum = CIFAR10_TAR_MD5\n",
        "    elif dataset in ['cifar100', 'cifar100superclass']:\n",
        "        cifar = Path(CIFAR100_TAR_FILENAME)\n",
        "        md5sum = CIFAR100_TAR_MD5\n",
        "\n",
        "    if not cifar.is_file():\n",
        "        logger.warning(\"{} does not exists.\".format(cifar))\n",
        "        download_cifar(dataset)\n",
        "\n",
        "    cifar_md5sum = hashlib.md5(cifar.open('rb').read()).hexdigest()\n",
        "    if md5sum != cifar_md5sum:\n",
        "        logger.error(\"File `{0}` may be corrupted (wrong md5 checksum). Please delete `{0}` and retry\".format(cifar))\n",
        "        sys.exit(1)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_data_params(dataset):\n",
        "    if dataset == 'cifar10':\n",
        "        TARFILE = CIFAR10_TAR_FILENAME\n",
        "        label_data = 'data'\n",
        "        label_labels = 'labels'\n",
        "        label_coarse = None\n",
        "    elif dataset == 'cifar100':\n",
        "        TARFILE = CIFAR100_TAR_FILENAME\n",
        "        label_data = 'data'\n",
        "        label_labels = 'fine_labels'\n",
        "        label_coarse = None\n",
        "    elif dataset == 'cifar100superclass':\n",
        "        TARFILE = CIFAR100_TAR_FILENAME\n",
        "        label_data = 'data'\n",
        "        label_labels = 'fine_labels'\n",
        "        label_coarse = 'coarse_labels'\n",
        "    return TARFILE, label_data, label_labels, label_coarse\n",
        "\n",
        "\n",
        "def get_datanames(dataset, mode):\n",
        "    if dataset == 'cifar10':\n",
        "        if mode == 'train':\n",
        "            return CIFAR10_TRAIN_DATA_NAMES\n",
        "        elif mode == 'test':\n",
        "            return CIFAR10_TEST_DATA_NAMES\n",
        "    elif dataset in ['cifar100', 'cifar100superclass']:\n",
        "        if mode == 'train':\n",
        "            return CIFAR100_TRAIN_DATA_NAMES\n",
        "        elif mode == 'test':\n",
        "            return CIFAR100_TEST_DATA_NAMES\n",
        "\n",
        "\n",
        "def parse_cifar(dataset, mode):\n",
        "    features = []\n",
        "    labels = []\n",
        "    coarse_labels = []\n",
        "    batch_names = []\n",
        "\n",
        "    TARFILE, label_data, label_labels, label_coarse = get_data_params(dataset)\n",
        "    datanames = get_datanames(dataset, mode)\n",
        "\n",
        "    try:\n",
        "        spinner = Spinner(prefix=\"Loading {} data...\".format(mode))\n",
        "        spinner.start()\n",
        "        tf = tarfile.open(TARFILE)\n",
        "        for dataname in datanames:\n",
        "            ti = tf.getmember(dataname)\n",
        "            data = unpickle(tf.extractfile(ti))\n",
        "            features.append(data[label_data])\n",
        "            labels.append(data[label_labels])\n",
        "            batch_names.extend([dataname.split('/')[1]] * len(data[label_data]))\n",
        "            if dataset == 'cifar100superclass':\n",
        "                coarse_labels.append(data[label_coarse])\n",
        "        features = np.concatenate(features)\n",
        "        features = features.reshape(features.shape[0], 3, 32, 32)\n",
        "        features = features.transpose(0, 2, 3, 1).astype('uint8')\n",
        "        labels = np.concatenate(labels)\n",
        "        if dataset == 'cifar100superclass':\n",
        "            coarse_labels = np.concatenate(coarse_labels)\n",
        "        spinner.stop()\n",
        "    except KeyboardInterrupt:\n",
        "        spinner.stop()\n",
        "        sys.exit(1)\n",
        "\n",
        "    return features, labels, coarse_labels, batch_names\n",
        "\n",
        "\n",
        "def save_cifar():\n",
        "    dataset = \"cifar100\"\n",
        "    output = \"./train_data\"\n",
        "    if dataset == 'cifar10':\n",
        "        LABELS = CIFAR10_LABELS_LIST\n",
        "        LABELS_LIST = CIFAR10_LABELS_LIST\n",
        "    elif dataset == 'cifar100':\n",
        "        LABELS = CIFAR100_LABELS_LIST\n",
        "        LABELS_LIST = CIFAR100_LABELS_LIST\n",
        "    elif dataset == 'cifar100superclass':\n",
        "        LABELS = []\n",
        "        for i in zip(CIFAR100_SUPERCLASS_LABELS_LIST, CIFAR100_CLASSES_LABELS_LIST):\n",
        "            for j in product([i[0]], i[1]):\n",
        "                LABELS.append('/'.join(j))\n",
        "        LABELS_LIST = CIFAR100_LABELS_LIST\n",
        "        COARSE_LABELS_LIST = CIFAR100_SUPERCLASS_LABELS_LIST\n",
        "\n",
        "    for mode in ['train', 'test']:\n",
        "        for label in LABELS:\n",
        "            dirpath = os.path.join(output, mode, label)\n",
        "            os.system(\"mkdir -p {}\".format(dirpath))\n",
        "\n",
        "        features, labels , coarse_labels, batch_names = parse_cifar(dataset, mode)\n",
        "\n",
        "        label_count = defaultdict(int)\n",
        "        batch_count = defaultdict(int)\n",
        "        for feature, label, coarse_label, batch_name in tqdm(zip_longest(features, labels, coarse_labels, batch_names), total=len(labels), desc=\"Saving {} images\".format(mode)):\n",
        "            label_count[label] += 1\n",
        "            if True:\n",
        "                if dataset == 'cifar10':\n",
        "                    filename = '%s_index_%04d.png' % (batch_name, batch_count[batch_name])\n",
        "                else:\n",
        "                    filename = '%s_index_%05d.png' % (batch_name, batch_count[batch_name])\n",
        "            else:\n",
        "                filename = '%04d.png' % label_count[label]\n",
        "            batch_count[batch_name] += 1\n",
        "\n",
        "            if dataset == 'cifar100superclass':\n",
        "                filepath = os.path.join(output, mode, COARSE_LABELS_LIST[coarse_label], LABELS_LIST[label], filename)\n",
        "            else:\n",
        "                filepath = os.path.join(output, mode, LABELS_LIST[label], filename)\n",
        "            image = Image.fromarray(feature)\n",
        "            image = image.convert('RGB')\n",
        "            image.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO3-wRDjckZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e29c877-447c-46c3-d15b-0148cc5f66b5"
      },
      "source": [
        "download_cifar('cifar100')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading cifar-100-python.tar.gz\n",
            "165kKB [00:03, 43.3kKB/s]                          \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhfEKIcckXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "417e5a12-4ff0-4c26-ba74-eea3ff4a865f"
      },
      "source": [
        "save_cifar()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train images: 100%|██████████| 50000/50000 [00:17<00:00, 2797.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test images: 100%|██████████| 10000/10000 [00:03<00:00, 2970.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkViRgN3hEe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "fname='cifar-100-python.tar.gz'\n",
        "if (fname.endswith(\"tar.gz\")):\n",
        "    tar = tarfile.open(fname, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "elif (fname.endswith(\"tar\")):\n",
        "    tar = tarfile.open(fname, \"r:\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVKN8wiRCmLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS_8LIcEjQjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "593ffaad-6afb-4f83-a776-ed5c8f9a6b51"
      },
      "source": [
        "from six.moves import cPickle\n",
        "f = open(\"cifar-100-python/meta\", 'rb')\n",
        "datadict = cPickle.load(f,encoding='latin1')\n",
        "f.close()\n",
        "X = datadict[\"data\"]\n",
        "Y = datadict['labels']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a1e50e8ccbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0XLiQXsjRrW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "9dc89843-5392-4588-930d-c654f3c3a317"
      },
      "source": [
        "unpickle('cifar-100-python.tar.gz')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-27d33759a9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-100-python.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-236de0d9a471>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x1f'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl-SShwRjRoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mmHzZhwa45N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "0e96f014-01dd-4c1f-b2aa-648f00751f6e"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras import applications\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Loading the Dataset\n",
        "with open('cifar-100-python/train', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    train_label= pickle.load(f)\n",
        "with open('cifar-100-python/test', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "#Residual Network with ILSVRC weights\n",
        "base_model = applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
        "#Debug\n",
        "base_model.summary()\n",
        "#Extract from the average pooling layer\n",
        "layers_to_extract = [\"avg_pool\"]\n",
        "\n",
        "#Select the features from average pooling layer\n",
        "model = Model(input=base_model.input, output=base_model.get_layer(layers_to_extract[0]).output)\n",
        "\n",
        "\n",
        "#To extract the features from the selected layer of ResNet50 Net\n",
        "layer_num=0\n",
        "feats=[]\n",
        "for img_count in range (0,50000): #Change to 10000 for test data\n",
        "\tprint(img_count)\n",
        "\t\n",
        "\t#Pre-processing\n",
        "\timage1 = np.zeros((32,32,3),dtype=np.uint8)\n",
        "\timage1[...,0] = np.reshape(train_data[img_count,:1024],(32,32)) #replace with test_data for test data features\n",
        "\timage1[...,1] = np.reshape(train_data[img_count,1024:2048],(32,32)) #replace with test_data for test data features\n",
        "\timage1[...,2] = np.reshape(train_data[img_count,2048:3072],(32,32)) #replace with test_data for test data features\n",
        "\timage1 = cv2.resize(image1,(224,224))\n",
        "\tx_in = image.img_to_array(image1)\n",
        "\tx_in = np.expand_dims(x_in, axis=0)\n",
        "\tx_in = preprocess_input(x_in)\n",
        "\t\n",
        "\t#Feature Extraction\n",
        "\tfeatures = model.predict(x_in)\n",
        "\tfeatures = features.flatten()\n",
        "\tfeats.append(features)\n",
        "\tfeatures_arr = np.char.mod('%f', features)\n",
        "\t\n",
        "feature_list = np.squeeze(np.asarray(feats))\n",
        "np.save(\"train_data\"+layers_to_extract[layer_num]+\"resnet_data.npy\",feature_list)\n",
        "\n",
        "# Test Dataset\n",
        "\n",
        "layer_num=0\n",
        "feats=[]\n",
        "for img_count in range (0,10000): #Change to 10000 for test data\n",
        "\tprint(img_count)\n",
        "\t\n",
        "\t#Pre-processing\n",
        "\timage1 = np.zeros((32,32,3),dtype=np.uint8)\n",
        "\timage1[...,0] = np.reshape(test_data[img_count,:1024],(32,32)) #replaced with test_data for test data features\n",
        "\timage1[...,1] = np.reshape(test_data[img_count,1024:2048],(32,32)) #replaced with test_data for test data features\n",
        "\timage1[...,2] = np.reshape(test_data[img_count,2048:3072],(32,32)) #replaced with test_data for test data features\n",
        "\timage1 = cv2.resize(image1,(224,224))\n",
        "\tx_in = image.img_to_array(image1)\n",
        "\tx_in = np.expand_dims(x_in, axis=0)\n",
        "\tx_in = preprocess_input(x_in)\n",
        "\t\n",
        "\t#Feature Extraction\n",
        "\tfeatures = model.predict(x_in)\n",
        "\tfeatures = features.flatten()\n",
        "\tfeats.append(features)\n",
        "\tfeatures_arr = np.char.mod('%f', features)\n",
        "\t\n",
        "feature_list = np.squeeze(np.asarray(feats))\n",
        "np.save(\"test_data\"+layers_to_extract[layer_num]+\"resnet_data.npy\",feature_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-77f6d20f1c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Loading the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-100-python/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-100-python/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGIx-6mvbHwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,Activation,BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import AveragePooling2D\n",
        "\n",
        "#Loading the Features\n",
        "X_train = np.load(\"train_dataavg_poolresnet_data.npy\")\n",
        "X_test = np.load(\"test_dataavg_poolresnet_data.npy\")\n",
        "\n",
        "#Loading the labels of training data\n",
        "with open('train_data/train_data', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    train_label= pickle.load(f)\n",
        "\n",
        "#Convert the labels to one-hot encoding\n",
        "trainImageLabels = keras.utils.to_categorical(train_label, num_classes=100)\n",
        "\n",
        "#Train Test Split 80%-20%\n",
        "x_tr,x_ts,y_tr,y_ts = train_test_split(X_train, trainImageLabels, test_size=0.2,random_state=1)\n",
        "\n",
        "#Creating a Deep Model for classifying the features of CIFAR-100 extracted from ResNet-50 trained on ILSVRC Dataset\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3,border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Convolution2D(32, 1, 1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(64, 3, 3,border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(128, 3, 3,border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Convolution2D(32, 1, 1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(256, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1))\n",
        "\n",
        "model.add(AveragePooling2D(pool_size = (4,4)))\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_tr,y_tr,batch_size=128,epochs=30,validation_data=(x_ts, y_ts),verbose=1)\n",
        "\n",
        "#Validation Accuracy\n",
        "score = model.evaluate(x_ts, y_ts,verbose=1)\n",
        "print(score)\n",
        "\n",
        "#Save the trained Model\n",
        "model.save('trainedModel.h5')\n",
        "\n",
        "#Classify the test data (Submission files have been trained with 100% of training data after validating on 80-20)\n",
        "predictions_ts = (model.predict(X_test))\n",
        "class_result=np.argmax(predictions_ts,axis=-1)\n",
        "\n",
        "np.savetxt(\"submission_labels.csv\", class_result, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zPtE5TibHse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPxx95KmbHqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}