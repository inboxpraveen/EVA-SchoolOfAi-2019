{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 13 - Cifar 10 with ResNet & 92% ValAcc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr3cJnfqhUaM",
        "colab_type": "code",
        "outputId": "2d5738de-fea7-48f6-fc8e-05864e28311f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras import regularizers, initializers\n",
        "from keras.layers.advanced_activations import LeakyReLU, ReLU, Softmax\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow, imsave\n",
        "import imageio"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8F1cNgxhQsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-1USuLIhMFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_bn_relu(input, block_no):\n",
        "  ch_in = 16*(2**block_no)\n",
        "  c1 = Conv2D(ch_in, (3,3),\n",
        "              kernel_regularizer=regularizers.l2(5e-4),  # weight decay = 5e-4\n",
        "              kernel_initializer='glorot_uniform',\n",
        "              padding='same', \n",
        "              name='begin_block{}_conv1'.format(block_no),\n",
        "              use_bias=False)(input)\n",
        "  c1 = BatchNormalization(name='begin_block{}_norm1'.format(block_no))(c1)\n",
        "  c1 = ReLU()(c1)\n",
        "  \n",
        "  return c1\n",
        "\n",
        "# Custom ResBlock\n",
        "def add_resblock(input, dropout_rate = None, layers=2, block_no=1, first_block=False, final_block=False):\n",
        "  ch_in = input.shape[3]*2\n",
        "  temp = input\n",
        "  \n",
        "  for l in range(layers):\n",
        "    c1 = Conv2D(int(ch_in*(2**l)),\n",
        "                (3,3),\n",
        "                kernel_regularizer=regularizers.l2(0.001),\n",
        "#                 kernel_initializer='glorot_uniform',\n",
        "                use_bias=False,\n",
        "                padding='same', \n",
        "                name = 'res_block{}_conv{}'.format(block_no, l+1))(temp)\n",
        "    if dropout_rate!=None:\n",
        "      c1 = Dropout(dropout_rate)(c1)\n",
        "      \n",
        "    c1 = BatchNormalization(name = 'res_block{}_BN{}'.format(block_no, l+1))(c1)\n",
        "        \n",
        "    c1 = ReLU(name = 'res_block{}_relu{}'.format(block_no, l+1))(c1)\n",
        "    \n",
        "    temp = c1\n",
        "\n",
        "    \n",
        "  concat = Concatenate(axis=-1)([input,temp])\n",
        "\n",
        "\n",
        "  if not final_block:\n",
        "\n",
        "    tr_layer = Conv2D(int(ch_in), (1,1),\n",
        "                      kernel_regularizer=regularizers.l2(0.001),\n",
        "                      use_bias=False,\n",
        "                      padding='same', \n",
        "                      name = 'res_block{}_transition'.format(block_no))(concat)\n",
        "    tr_layer = BatchNormalization(name = 'res_block_transition1x1{}_BN'.format(block_no))(tr_layer)\n",
        "    tr_layer = ReLU(name = 'res_block_transition1x1{}_relu'.format(block_no))(tr_layer)\n",
        "    \n",
        "    \n",
        "    return MaxPooling2D(pool_size=(2, 2))(tr_layer)\n",
        "  else:\n",
        "    return concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS_bZA2yq6ay",
        "colab_type": "text"
      },
      "source": [
        "## Conv -> B1 -> B2 -> B3 -> B4 -> output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUciQ-u2hMCu",
        "colab_type": "code",
        "outputId": "4d8d11e3-ce34-4060-a661-7a32c64c2b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input = Input(shape=(32, 32, 3,))\n",
        "\n",
        "First_Conv2D = conv_bn_relu(input, block_no=1)\n",
        "# second_Conv2D = conv_bn_relu(First_Conv2D, block_no=2)\n",
        "# first_maxpool = MaxPooling2D(pool_size=(2, 2), name = 'startmaxpool')(First_Conv2D)\n",
        "\n",
        "block1 = add_resblock(First_Conv2D, layers=2, block_no=1, first_block=True)\n",
        "\n",
        "block2 = add_resblock(block1, layers=2, block_no=2)\n",
        "\n",
        "block3 = add_resblock(block2, layers=2, block_no=3)\n",
        "\n",
        "block4 = add_resblock(block3, layers=2, block_no=4, final_block=True)\n",
        "\n",
        "reduce_ch = Conv2D(num_classes, (1,1), name='number_of_classes', use_bias=False)(block4)\n",
        "avg_pool = GlobalAveragePooling2D()(reduce_ch)\n",
        "output = Softmax()(avg_pool)\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "begin_block1_conv1 (Conv2D)     (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "begin_block1_norm1 (BatchNormal (None, 32, 32, 32)   128         begin_block1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 32, 32, 32)   0           begin_block1_norm1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_conv1 (Conv2D)       (None, 32, 32, 64)   18432       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_BN1 (BatchNormalizat (None, 32, 32, 64)   256         res_block1_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_relu1 (ReLU)         (None, 32, 32, 64)   0           res_block1_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_conv2 (Conv2D)       (None, 32, 32, 128)  73728       res_block1_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_BN2 (BatchNormalizat (None, 32, 32, 128)  512         res_block1_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_relu2 (ReLU)         (None, 32, 32, 128)  0           res_block1_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 160)  0           re_lu_1[0][0]                    \n",
            "                                                                 res_block1_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_transition (Conv2D)  (None, 32, 32, 64)   10240       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x11_BN (Ba (None, 32, 32, 64)   256         res_block1_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x11_relu ( (None, 32, 32, 64)   0           res_block_transition1x11_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           res_block_transition1x11_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block2_conv1 (Conv2D)       (None, 16, 16, 128)  73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_BN1 (BatchNormalizat (None, 16, 16, 128)  512         res_block2_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_relu1 (ReLU)         (None, 16, 16, 128)  0           res_block2_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_conv2 (Conv2D)       (None, 16, 16, 256)  294912      res_block2_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_BN2 (BatchNormalizat (None, 16, 16, 256)  1024        res_block2_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_relu2 (ReLU)         (None, 16, 16, 256)  0           res_block2_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 16, 16, 320)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 res_block2_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_transition (Conv2D)  (None, 16, 16, 128)  40960       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x12_BN (Ba (None, 16, 16, 128)  512         res_block2_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x12_relu ( (None, 16, 16, 128)  0           res_block_transition1x12_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           res_block_transition1x12_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block3_conv1 (Conv2D)       (None, 8, 8, 256)    294912      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_BN1 (BatchNormalizat (None, 8, 8, 256)    1024        res_block3_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_relu1 (ReLU)         (None, 8, 8, 256)    0           res_block3_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_conv2 (Conv2D)       (None, 8, 8, 512)    1179648     res_block3_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_BN2 (BatchNormalizat (None, 8, 8, 512)    2048        res_block3_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_relu2 (ReLU)         (None, 8, 8, 512)    0           res_block3_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 8, 8, 640)    0           max_pooling2d_2[0][0]            \n",
            "                                                                 res_block3_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_transition (Conv2D)  (None, 8, 8, 256)    163840      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x13_BN (Ba (None, 8, 8, 256)    1024        res_block3_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x13_relu ( (None, 8, 8, 256)    0           res_block_transition1x13_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 256)    0           res_block_transition1x13_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block4_conv1 (Conv2D)       (None, 4, 4, 512)    1179648     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_BN1 (BatchNormalizat (None, 4, 4, 512)    2048        res_block4_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_relu1 (ReLU)         (None, 4, 4, 512)    0           res_block4_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_conv2 (Conv2D)       (None, 4, 4, 1024)   4718592     res_block4_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_BN2 (BatchNormalizat (None, 4, 4, 1024)   4096        res_block4_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_relu2 (ReLU)         (None, 4, 4, 1024)   0           res_block4_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 4, 4, 1280)   0           max_pooling2d_3[0][0]            \n",
            "                                                                 res_block4_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "number_of_classes (Conv2D)      (None, 4, 4, 10)     12800       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           number_of_classes[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,075,744\n",
            "Trainable params: 8,069,024\n",
            "Non-trainable params: 6,720\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TyLgZfuhjz_",
        "colab_type": "code",
        "outputId": "3c668311-378a-4823-b802-8bcfb060c67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXmgrXUqhovs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras as k\n",
        "y_train = k.utils.to_categorical(y_train, num_classes)\n",
        "y_test = k.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhG-hNrihtZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yqao4qujHys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = CyclicLR(base_lr=0.01, max_lr=0.1,step_size=780.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65OMUNoS3Nu",
        "colab_type": "text"
      },
      "source": [
        "## clr with simple augmentations .. .trial one\n",
        "\n",
        "\n",
        "max acc : 89% obtained...\n",
        "\n",
        "\n",
        "need tweaking with augmentations and lr scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeZ-p4zQh3xx",
        "colab_type": "code",
        "outputId": "970cc7d9-f16d-416d-d04e-2826312c2b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#SDG \n",
        "sgd = SGD(lr=0.01, momentum = 0.9,nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        "    )\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGoHImT7idHX",
        "colab_type": "code",
        "outputId": "0d432250-d648-4c40-ce0a-4ddf5e1dcd2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#fit\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch = len(x_train) / 128, epochs=epochs, validation_data=(x_test, y_test),callbacks=[clr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "391/390 [==============================] - 38s 98ms/step - loss: 3.5353 - acc: 0.5314 - val_loss: 5.3472 - val_acc: 0.2827\n",
            "Epoch 2/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 2.2281 - acc: 0.6690 - val_loss: 2.0407 - val_acc: 0.6118\n",
            "Epoch 3/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 1.4079 - acc: 0.7374 - val_loss: 1.9759 - val_acc: 0.4870\n",
            "Epoch 4/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 0.9900 - acc: 0.8075 - val_loss: 1.2171 - val_acc: 0.7169\n",
            "Epoch 5/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 0.8898 - acc: 0.8194 - val_loss: 1.7164 - val_acc: 0.5635\n",
            "Epoch 6/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 1.0902 - acc: 0.7653 - val_loss: 1.9460 - val_acc: 0.6084\n",
            "Epoch 7/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0736 - acc: 0.7845 - val_loss: 1.3363 - val_acc: 0.6603\n",
            "Epoch 8/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 0.8384 - acc: 0.8404 - val_loss: 0.8224 - val_acc: 0.8325\n",
            "Epoch 9/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7754 - acc: 0.8462 - val_loss: 1.1939 - val_acc: 0.7266\n",
            "Epoch 10/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 1.0208 - acc: 0.7902 - val_loss: 1.6338 - val_acc: 0.6403\n",
            "Epoch 11/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 1.0482 - acc: 0.8017 - val_loss: 1.5218 - val_acc: 0.6540\n",
            "Epoch 12/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.8155 - acc: 0.8538 - val_loss: 0.7803 - val_acc: 0.8558\n",
            "Epoch 13/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 0.7556 - acc: 0.8583 - val_loss: 2.3126 - val_acc: 0.5666\n",
            "Epoch 14/100\n",
            "391/390 [==============================] - 30s 76ms/step - loss: 1.0168 - acc: 0.8012 - val_loss: 1.8005 - val_acc: 0.6218\n",
            "Epoch 15/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0453 - acc: 0.8095 - val_loss: 1.2467 - val_acc: 0.7269\n",
            "Epoch 16/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.8076 - acc: 0.8628 - val_loss: 0.7413 - val_acc: 0.8726\n",
            "Epoch 17/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7462 - acc: 0.8657 - val_loss: 1.5535 - val_acc: 0.6510\n",
            "Epoch 18/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0067 - acc: 0.8075 - val_loss: 3.6258 - val_acc: 0.4374\n",
            "Epoch 19/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0341 - acc: 0.8201 - val_loss: 1.0811 - val_acc: 0.7776\n",
            "Epoch 20/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.8040 - acc: 0.8679 - val_loss: 0.7489 - val_acc: 0.8749\n",
            "Epoch 21/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7440 - acc: 0.8690 - val_loss: 1.6730 - val_acc: 0.6728\n",
            "Epoch 22/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0167 - acc: 0.8099 - val_loss: 4.6991 - val_acc: 0.3076\n",
            "Epoch 23/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0374 - acc: 0.8192 - val_loss: 1.0629 - val_acc: 0.7963\n",
            "Epoch 24/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7988 - acc: 0.8712 - val_loss: 0.7456 - val_acc: 0.8742\n",
            "Epoch 25/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7421 - acc: 0.8699 - val_loss: 1.3484 - val_acc: 0.7036\n",
            "Epoch 26/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0067 - acc: 0.8146 - val_loss: 2.3520 - val_acc: 0.5206\n",
            "Epoch 27/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0346 - acc: 0.8261 - val_loss: 1.3566 - val_acc: 0.7050\n",
            "Epoch 28/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7960 - acc: 0.8734 - val_loss: 0.7751 - val_acc: 0.8733\n",
            "Epoch 29/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7420 - acc: 0.8732 - val_loss: 2.6631 - val_acc: 0.4850\n",
            "Epoch 30/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0154 - acc: 0.8151 - val_loss: 2.0504 - val_acc: 0.6104\n",
            "Epoch 31/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0361 - acc: 0.8271 - val_loss: 1.1336 - val_acc: 0.7768\n",
            "Epoch 32/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7840 - acc: 0.8772 - val_loss: 0.7909 - val_acc: 0.8630\n",
            "Epoch 33/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7381 - acc: 0.8753 - val_loss: 1.3203 - val_acc: 0.7160\n",
            "Epoch 34/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0086 - acc: 0.8201 - val_loss: 2.3553 - val_acc: 0.3622\n",
            "Epoch 35/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0329 - acc: 0.8292 - val_loss: 1.0976 - val_acc: 0.7954\n",
            "Epoch 36/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 0.7872 - acc: 0.8799 - val_loss: 0.7855 - val_acc: 0.8680\n",
            "Epoch 37/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7402 - acc: 0.8757 - val_loss: 1.7535 - val_acc: 0.6130\n",
            "Epoch 38/100\n",
            "391/390 [==============================] - 29s 75ms/step - loss: 1.0138 - acc: 0.8196 - val_loss: 1.8544 - val_acc: 0.5917\n",
            "Epoch 39/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0255 - acc: 0.8320 - val_loss: 1.1631 - val_acc: 0.7688\n",
            "Epoch 40/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7772 - acc: 0.8825 - val_loss: 0.7622 - val_acc: 0.8735\n",
            "Epoch 41/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7438 - acc: 0.8749 - val_loss: 1.5507 - val_acc: 0.6458\n",
            "Epoch 42/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0254 - acc: 0.8177 - val_loss: 2.1801 - val_acc: 0.5471\n",
            "Epoch 43/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0218 - acc: 0.8355 - val_loss: 1.3849 - val_acc: 0.7037\n",
            "Epoch 44/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7776 - acc: 0.8842 - val_loss: 0.7594 - val_acc: 0.8721\n",
            "Epoch 45/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7464 - acc: 0.8759 - val_loss: 2.4686 - val_acc: 0.4477\n",
            "Epoch 46/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0311 - acc: 0.8197 - val_loss: 1.5513 - val_acc: 0.6615\n",
            "Epoch 47/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0273 - acc: 0.8363 - val_loss: 1.0726 - val_acc: 0.7982\n",
            "Epoch 48/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7765 - acc: 0.8856 - val_loss: 0.8589 - val_acc: 0.8493\n",
            "Epoch 49/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7448 - acc: 0.8774 - val_loss: 1.8624 - val_acc: 0.5702\n",
            "Epoch 50/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0328 - acc: 0.8193 - val_loss: 2.3612 - val_acc: 0.4518\n",
            "Epoch 51/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0341 - acc: 0.8364 - val_loss: 1.9951 - val_acc: 0.4934\n",
            "Epoch 52/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7759 - acc: 0.8859 - val_loss: 0.7425 - val_acc: 0.8839\n",
            "Epoch 53/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7534 - acc: 0.8770 - val_loss: 1.3611 - val_acc: 0.6904\n",
            "Epoch 54/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0466 - acc: 0.8200 - val_loss: 3.0726 - val_acc: 0.4483\n",
            "Epoch 55/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0311 - acc: 0.8381 - val_loss: 1.1539 - val_acc: 0.7657\n",
            "Epoch 56/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7773 - acc: 0.8888 - val_loss: 0.7364 - val_acc: 0.8875\n",
            "Epoch 57/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7548 - acc: 0.8788 - val_loss: 2.8820 - val_acc: 0.4914\n",
            "Epoch 58/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0392 - acc: 0.8232 - val_loss: 1.4685 - val_acc: 0.7035\n",
            "Epoch 59/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0312 - acc: 0.8393 - val_loss: 1.1237 - val_acc: 0.7892\n",
            "Epoch 60/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7762 - acc: 0.8883 - val_loss: 0.7892 - val_acc: 0.8721\n",
            "Epoch 61/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7549 - acc: 0.8774 - val_loss: 1.6340 - val_acc: 0.6296\n",
            "Epoch 62/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0572 - acc: 0.8227 - val_loss: 1.8806 - val_acc: 0.6176\n",
            "Epoch 63/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0419 - acc: 0.8375 - val_loss: 1.4540 - val_acc: 0.6839\n",
            "Epoch 64/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7703 - acc: 0.8912 - val_loss: 0.7706 - val_acc: 0.8784\n",
            "Epoch 65/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7608 - acc: 0.8773 - val_loss: 1.9023 - val_acc: 0.6339\n",
            "Epoch 66/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0536 - acc: 0.8204 - val_loss: 1.7791 - val_acc: 0.6358\n",
            "Epoch 67/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0332 - acc: 0.8407 - val_loss: 1.0476 - val_acc: 0.8143\n",
            "Epoch 68/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7789 - acc: 0.8904 - val_loss: 0.7925 - val_acc: 0.8704\n",
            "Epoch 69/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7699 - acc: 0.8768 - val_loss: 2.8922 - val_acc: 0.4557\n",
            "Epoch 70/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 1.0571 - acc: 0.8212 - val_loss: 1.6707 - val_acc: 0.6540\n",
            "Epoch 71/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0368 - acc: 0.8418 - val_loss: 1.1335 - val_acc: 0.7905\n",
            "Epoch 72/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7714 - acc: 0.8935 - val_loss: 0.8127 - val_acc: 0.8639\n",
            "Epoch 73/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7653 - acc: 0.8779 - val_loss: 2.2713 - val_acc: 0.5212\n",
            "Epoch 74/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0645 - acc: 0.8228 - val_loss: 2.4043 - val_acc: 0.4744\n",
            "Epoch 75/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 1.0299 - acc: 0.8447 - val_loss: 1.2580 - val_acc: 0.7513\n",
            "Epoch 76/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7755 - acc: 0.8924 - val_loss: 0.7450 - val_acc: 0.8868\n",
            "Epoch 77/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7681 - acc: 0.8771 - val_loss: 1.5615 - val_acc: 0.6725\n",
            "Epoch 78/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0687 - acc: 0.8233 - val_loss: 1.8193 - val_acc: 0.5963\n",
            "Epoch 79/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0324 - acc: 0.8442 - val_loss: 1.0459 - val_acc: 0.8197\n",
            "Epoch 80/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7683 - acc: 0.8954 - val_loss: 0.7977 - val_acc: 0.8669\n",
            "Epoch 81/100\n",
            "391/390 [==============================] - 29s 74ms/step - loss: 0.7742 - acc: 0.8770 - val_loss: 2.1590 - val_acc: 0.4957\n",
            "Epoch 82/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0693 - acc: 0.8241 - val_loss: 2.9342 - val_acc: 0.4384\n",
            "Epoch 83/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0403 - acc: 0.8452 - val_loss: 1.0979 - val_acc: 0.8015\n",
            "Epoch 84/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7700 - acc: 0.8963 - val_loss: 0.8104 - val_acc: 0.8686\n",
            "Epoch 85/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7810 - acc: 0.8755 - val_loss: 2.3743 - val_acc: 0.5260\n",
            "Epoch 86/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 1.0821 - acc: 0.8225 - val_loss: 1.6499 - val_acc: 0.6575\n",
            "Epoch 87/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 1.0369 - acc: 0.8465 - val_loss: 1.1437 - val_acc: 0.7904\n",
            "Epoch 88/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7729 - acc: 0.8936 - val_loss: 0.7798 - val_acc: 0.8797\n",
            "Epoch 89/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7777 - acc: 0.8764 - val_loss: 1.5639 - val_acc: 0.6717\n",
            "Epoch 90/100\n",
            "391/390 [==============================] - 28s 72ms/step - loss: 1.0833 - acc: 0.8239 - val_loss: 1.4582 - val_acc: 0.7257\n",
            "Epoch 91/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 1.0354 - acc: 0.8472 - val_loss: 1.1354 - val_acc: 0.7927\n",
            "Epoch 92/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7764 - acc: 0.8955 - val_loss: 0.7818 - val_acc: 0.8810\n",
            "Epoch 93/100\n",
            "391/390 [==============================] - 28s 72ms/step - loss: 0.7924 - acc: 0.8742 - val_loss: 3.1656 - val_acc: 0.3706\n",
            "Epoch 94/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 1.0930 - acc: 0.8230 - val_loss: 1.6869 - val_acc: 0.6708\n",
            "Epoch 95/100\n",
            "391/390 [==============================] - 28s 72ms/step - loss: 1.0357 - acc: 0.8441 - val_loss: 1.0694 - val_acc: 0.8193\n",
            "Epoch 96/100\n",
            "391/390 [==============================] - 28s 72ms/step - loss: 0.7669 - acc: 0.8982 - val_loss: 0.7898 - val_acc: 0.8747\n",
            "Epoch 97/100\n",
            "391/390 [==============================] - 29s 73ms/step - loss: 0.7912 - acc: 0.8751 - val_loss: 2.2339 - val_acc: 0.4654\n",
            "Epoch 98/100\n",
            "391/390 [==============================] - 28s 72ms/step - loss: 1.0993 - acc: 0.8217 - val_loss: 1.5802 - val_acc: 0.6891\n",
            "Epoch 99/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 1.0395 - acc: 0.8474 - val_loss: 1.0991 - val_acc: 0.8017\n",
            "Epoch 100/100\n",
            "391/390 [==============================] - 28s 73ms/step - loss: 0.7709 - acc: 0.8970 - val_loss: 0.7966 - val_acc: 0.8766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f2dbbfcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-0S-tgAJhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import keras.callbacks as callbacks\n",
        "import keras.utils.np_utils as kutils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuU2xhnvSUfZ",
        "colab_type": "text"
      },
      "source": [
        "## Random crop, normalization and padding 4 pix. \n",
        "### Do not re run... does not work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf-F4wHBIxoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(x, random_crop_size = (32,32), sync_seed=None):\n",
        "    np.random.seed(sync_seed)\n",
        "    w, h = x.shape[1], x.shape[2]\n",
        "    rangew = (w - random_crop_size[0]) // 2\n",
        "    rangeh = (h - random_crop_size[1]) // 2\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    return x[:, offsetw:offsetw+random_crop_size[0], offseth:offseth+random_crop_size[1]]\n",
        "\n",
        "def pad(x, pad=4):\n",
        "    return np.pad(x, ((0,0), (pad,pad),(pad,pad),(0,0)), mode='reflect')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoXmLw0bI3Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64  ## 128 produces OOM\n",
        "nb_epoch = 130\n",
        "img_rows, img_cols = 32, 32\n",
        "\n",
        "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "trainX = pad(trainX)\n",
        "testX = pad(testX)\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "trainX = (trainX - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "testX = testX.astype('float32')\n",
        "testX = (testX - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "\n",
        "trainY = kutils.to_categorical(trainY)\n",
        "testY = kutils.to_categorical(testY)\n",
        "\n",
        "generator = ImageDataGenerator(zca_epsilon=0,\n",
        "                               rescale = 1./255,\n",
        "                               horizontal_flip=True,\n",
        "                               fill_mode='reflect',)\n",
        "\n",
        "generator.fit(trainX, seed=0, augment=True)\n",
        "\n",
        "test_generator = ImageDataGenerator(zca_epsilon=0,\n",
        "                                    rescale = 1./255,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='reflect')\n",
        "\n",
        "test_generator.fit(testX, seed=0, augment=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk8O1iPELWf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.001, momentum = 0.9,nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqt-PAepLYrh",
        "colab_type": "code",
        "outputId": "a38defe1-3e6b-43bf-8a34-1396bf71624b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "begin_block1_conv1 (Conv2D)     (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "begin_block1_norm1 (BatchNormal (None, 32, 32, 32)   128         begin_block1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 32, 32, 32)   0           begin_block1_norm1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_conv1 (Conv2D)       (None, 32, 32, 64)   18432       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_BN1 (BatchNormalizat (None, 32, 32, 64)   256         res_block1_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_relu1 (ReLU)         (None, 32, 32, 64)   0           res_block1_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_conv2 (Conv2D)       (None, 32, 32, 128)  73728       res_block1_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_BN2 (BatchNormalizat (None, 32, 32, 128)  512         res_block1_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_relu2 (ReLU)         (None, 32, 32, 128)  0           res_block1_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 160)  0           re_lu_1[0][0]                    \n",
            "                                                                 res_block1_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block1_transition (Conv2D)  (None, 32, 32, 64)   10240       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x11_BN (Ba (None, 32, 32, 64)   256         res_block1_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x11_relu ( (None, 32, 32, 64)   0           res_block_transition1x11_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           res_block_transition1x11_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block2_conv1 (Conv2D)       (None, 16, 16, 128)  73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_BN1 (BatchNormalizat (None, 16, 16, 128)  512         res_block2_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_relu1 (ReLU)         (None, 16, 16, 128)  0           res_block2_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_conv2 (Conv2D)       (None, 16, 16, 256)  294912      res_block2_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_BN2 (BatchNormalizat (None, 16, 16, 256)  1024        res_block2_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_relu2 (ReLU)         (None, 16, 16, 256)  0           res_block2_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 16, 16, 320)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 res_block2_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block2_transition (Conv2D)  (None, 16, 16, 128)  40960       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x12_BN (Ba (None, 16, 16, 128)  512         res_block2_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x12_relu ( (None, 16, 16, 128)  0           res_block_transition1x12_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           res_block_transition1x12_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block3_conv1 (Conv2D)       (None, 8, 8, 256)    294912      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_BN1 (BatchNormalizat (None, 8, 8, 256)    1024        res_block3_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_relu1 (ReLU)         (None, 8, 8, 256)    0           res_block3_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_conv2 (Conv2D)       (None, 8, 8, 512)    1179648     res_block3_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_BN2 (BatchNormalizat (None, 8, 8, 512)    2048        res_block3_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_relu2 (ReLU)         (None, 8, 8, 512)    0           res_block3_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 8, 8, 640)    0           max_pooling2d_2[0][0]            \n",
            "                                                                 res_block3_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block3_transition (Conv2D)  (None, 8, 8, 256)    163840      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x13_BN (Ba (None, 8, 8, 256)    1024        res_block3_transition[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "res_block_transition1x13_relu ( (None, 8, 8, 256)    0           res_block_transition1x13_BN[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 256)    0           res_block_transition1x13_relu[0][\n",
            "__________________________________________________________________________________________________\n",
            "res_block4_conv1 (Conv2D)       (None, 4, 4, 512)    1179648     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_BN1 (BatchNormalizat (None, 4, 4, 512)    2048        res_block4_conv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_relu1 (ReLU)         (None, 4, 4, 512)    0           res_block4_BN1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_conv2 (Conv2D)       (None, 4, 4, 1024)   4718592     res_block4_relu1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_BN2 (BatchNormalizat (None, 4, 4, 1024)   4096        res_block4_conv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res_block4_relu2 (ReLU)         (None, 4, 4, 1024)   0           res_block4_BN2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 4, 4, 1280)   0           max_pooling2d_3[0][0]            \n",
            "                                                                 res_block4_relu2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "number_of_classes (Conv2D)      (None, 4, 4, 10)     12800       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           number_of_classes[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,075,744\n",
            "Trainable params: 8,069,024\n",
            "Non-trainable params: 6,720\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUOSWHo0JSkz",
        "colab_type": "code",
        "outputId": "66652b3c-0b2a-4594-89f5-ca28fe816ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(0, nb_epoch):\n",
        "    print('Epoch = ' + str(i+1))\n",
        "    for x_batch, y_batch in generator.flow(trainX, trainY, batch_size=len(trainX)):\n",
        "        for testx_batch, testy_batch in test_generator.flow(testX, testY, batch_size=len(testX)):\n",
        "            x_batch = random_crop(x_batch)\n",
        "            testx_batch = random_crop(testx_batch)\n",
        "            model.fit(x_batch, y_batch, nb_epoch=1, callbacks=[clr], validation_data=(testx_batch, testy_batch))\n",
        "            break\n",
        "        break\n",
        "\n",
        "scores = model.evaluate_generator(test_generator.flow(testX, testY, nb_epoch), (testX.shape[0] / batch_size + 1))\n",
        "print(\"Accuracy = %f\" % (100 * scores[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch = 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 53s 1ms/step - loss: 2.5293 - acc: 0.5244 - val_loss: 1.3110 - val_acc: 0.6934\n",
            "Epoch = 2\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 888us/step - loss: 1.3333 - acc: 0.6961 - val_loss: 1.1526 - val_acc: 0.7526\n",
            "Epoch = 3\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 884us/step - loss: 1.2968 - acc: 0.7185 - val_loss: 1.2474 - val_acc: 0.7157\n",
            "Epoch = 4\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 879us/step - loss: 1.2901 - acc: 0.7249 - val_loss: 1.2535 - val_acc: 0.7152\n",
            "Epoch = 5\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 887us/step - loss: 1.2805 - acc: 0.7344 - val_loss: 1.1040 - val_acc: 0.7760\n",
            "Epoch = 6\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 883us/step - loss: 1.2588 - acc: 0.7444 - val_loss: 1.1514 - val_acc: 0.7619\n",
            "Epoch = 7\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 886us/step - loss: 1.2683 - acc: 0.7418 - val_loss: 1.1993 - val_acc: 0.7423\n",
            "Epoch = 8\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 885us/step - loss: 1.2491 - acc: 0.7523 - val_loss: 1.0990 - val_acc: 0.7825\n",
            "Epoch = 9\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 883us/step - loss: 1.3034 - acc: 0.7392 - val_loss: 1.2327 - val_acc: 0.7390\n",
            "Epoch = 10\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 878us/step - loss: 1.2684 - acc: 0.7493 - val_loss: 1.3004 - val_acc: 0.7194\n",
            "Epoch = 11\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 876us/step - loss: 1.2618 - acc: 0.7534 - val_loss: 1.0337 - val_acc: 0.8089\n",
            "Epoch = 12\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 881us/step - loss: 1.2699 - acc: 0.7536 - val_loss: 1.1610 - val_acc: 0.7631\n",
            "Epoch = 13\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 885us/step - loss: 1.3234 - acc: 0.7379 - val_loss: 1.0762 - val_acc: 0.8014\n",
            "Epoch = 14\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 885us/step - loss: 1.2887 - acc: 0.7500 - val_loss: 1.1289 - val_acc: 0.7858\n",
            "Epoch = 15\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 889us/step - loss: 1.2806 - acc: 0.7545 - val_loss: 1.1260 - val_acc: 0.7815\n",
            "Epoch = 16\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 878us/step - loss: 1.3071 - acc: 0.7443 - val_loss: 1.1577 - val_acc: 0.7676\n",
            "Epoch = 17\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 884us/step - loss: 1.2772 - acc: 0.7561 - val_loss: 1.0918 - val_acc: 0.7947\n",
            "Epoch = 18\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 882us/step - loss: 1.2756 - acc: 0.7560 - val_loss: 1.2012 - val_acc: 0.7546\n",
            "Epoch = 19\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 888us/step - loss: 1.3046 - acc: 0.7501 - val_loss: 1.2558 - val_acc: 0.7340\n",
            "Epoch = 20\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 882us/step - loss: 1.3069 - acc: 0.7476 - val_loss: 1.1191 - val_acc: 0.7841\n",
            "Epoch = 21\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 882us/step - loss: 1.2871 - acc: 0.7531 - val_loss: 1.2356 - val_acc: 0.7430\n",
            "Epoch = 22\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 877us/step - loss: 1.3031 - acc: 0.7500 - val_loss: 1.1271 - val_acc: 0.7826\n",
            "Epoch = 23\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 884us/step - loss: 1.2905 - acc: 0.7541 - val_loss: 1.2682 - val_acc: 0.7419\n",
            "Epoch = 24\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 885us/step - loss: 1.2782 - acc: 0.7587 - val_loss: 1.1670 - val_acc: 0.7701\n",
            "Epoch = 25\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 885us/step - loss: 1.3291 - acc: 0.7405 - val_loss: 1.1927 - val_acc: 0.7676\n",
            "Epoch = 26\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 890us/step - loss: 1.2959 - acc: 0.7529 - val_loss: 1.2006 - val_acc: 0.7652\n",
            "Epoch = 27\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 889us/step - loss: 1.2896 - acc: 0.7568 - val_loss: 1.1275 - val_acc: 0.7823\n",
            "Epoch = 28\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 44s 883us/step - loss: 1.2788 - acc: 0.7580 - val_loss: 1.2675 - val_acc: 0.7400\n",
            "Epoch = 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaye2DrofKZe",
        "colab_type": "text"
      },
      "source": [
        "#### Not working.. .stopped further epochs\n",
        "\n",
        "Saturates between 75 to 82%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsRay_9eSeWY",
        "colab_type": "text"
      },
      "source": [
        "## SGD Scheduler with other image augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4AL70pEoNPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "class SGDRScheduler(Callback):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=4,\n",
        "                 mult_factor=1):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpWoDtUwOEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "\n",
        "def other_augs(batches):\n",
        "  shape_seq = iaa.Sequential([\n",
        "    iaa.GaussianBlur(sigma=(0, 0.15)), # ex: 0.6\n",
        "    iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "    iaa.Sometimes(0.10, iaa.CropAndPad(percent=(0, .20))),\n",
        "    iaa.Sometimes(0.5,iaa.Affine(\n",
        "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
        "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
        "            rotate=(-10, 10), # rotate by -10 to +10 degrees\n",
        "            shear=(-10, 10) # shear by -10 to +10 degrees\n",
        "        )),\n",
        "  ])\n",
        "    \n",
        "  while True:\n",
        "    batch_x, batch_y = next(batches)\n",
        "    shape_augmented = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], batch_x.shape[3])) \n",
        "    shape_augmented = shape_seq.augment_images(batch_x)\n",
        "    yield (shape_augmented, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPo2MlcfwKjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = SGDRScheduler(0.01,\n",
        "                 0.1,\n",
        "                 390,\n",
        "                 lr_decay=0.09,\n",
        "                 cycle_length=4,\n",
        "                 mult_factor=1,\n",
        "                   )\n",
        "\n",
        "#SDG \n",
        "sgd = SGD(lr=0.01, momentum = 0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ux7RWGUwXE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeqbw0-Twxym",
        "colab_type": "code",
        "outputId": "359b7906-3da3-4df4-c8f4-329f0cf2cca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#fit\n",
        "model.fit_generator(other_augs(datagen.flow(x_train, y_train, batch_size=batch_size)),\n",
        "                    steps_per_epoch = len(x_train) / 128, epochs=epochs, validation_data=other_augs(datagen.flow(x_test, y_test,batch_size=batch_size)),validation_steps=len(x_test)/128,callbacks=[clr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "391/390 [==============================] - 30s 77ms/step - loss: 4.8054 - acc: 0.2276 - val_loss: 3.5431 - val_acc: 0.2055\n",
            "Epoch 2/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 3.0167 - acc: 0.3428 - val_loss: 2.8148 - val_acc: 0.2504\n",
            "Epoch 3/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 2.1267 - acc: 0.4613 - val_loss: 2.1354 - val_acc: 0.4183\n",
            "Epoch 4/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 1.6448 - acc: 0.5746 - val_loss: 1.8585 - val_acc: 0.4916\n",
            "Epoch 5/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 1.4388 - acc: 0.6311 - val_loss: 1.8741 - val_acc: 0.5127\n",
            "Epoch 6/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 1.3186 - acc: 0.6627 - val_loss: 1.6036 - val_acc: 0.5621\n",
            "Epoch 7/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 1.1991 - acc: 0.6966 - val_loss: 1.2667 - val_acc: 0.6721\n",
            "Epoch 8/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 1.0973 - acc: 0.7239 - val_loss: 1.2919 - val_acc: 0.6619\n",
            "Epoch 9/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.9628 - acc: 0.7691 - val_loss: 0.9946 - val_acc: 0.7578\n",
            "Epoch 10/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.9221 - acc: 0.7818 - val_loss: 1.0300 - val_acc: 0.7485\n",
            "Epoch 11/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.9355 - acc: 0.7726 - val_loss: 1.1612 - val_acc: 0.6961\n",
            "Epoch 12/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.9152 - acc: 0.7767 - val_loss: 1.0764 - val_acc: 0.7217\n",
            "Epoch 13/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.8821 - acc: 0.7881 - val_loss: 0.8634 - val_acc: 0.7885\n",
            "Epoch 14/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7754 - acc: 0.8198 - val_loss: 0.9620 - val_acc: 0.7594\n",
            "Epoch 15/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.8033 - acc: 0.8085 - val_loss: 1.0251 - val_acc: 0.7350\n",
            "Epoch 16/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.8232 - acc: 0.8000 - val_loss: 1.0164 - val_acc: 0.7348\n",
            "Epoch 17/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.8225 - acc: 0.8035 - val_loss: 0.7685 - val_acc: 0.8222\n",
            "Epoch 18/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6762 - acc: 0.8494 - val_loss: 0.9288 - val_acc: 0.7715\n",
            "Epoch 19/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7205 - acc: 0.8315 - val_loss: 1.0322 - val_acc: 0.7253\n",
            "Epoch 20/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7486 - acc: 0.8231 - val_loss: 0.9700 - val_acc: 0.7539\n",
            "Epoch 21/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7715 - acc: 0.8163 - val_loss: 0.7469 - val_acc: 0.8268\n",
            "Epoch 22/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6182 - acc: 0.8657 - val_loss: 0.7795 - val_acc: 0.8124\n",
            "Epoch 23/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6579 - acc: 0.8498 - val_loss: 0.9931 - val_acc: 0.7458\n",
            "Epoch 24/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6948 - acc: 0.8394 - val_loss: 1.2201 - val_acc: 0.7048\n",
            "Epoch 25/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7061 - acc: 0.8373 - val_loss: 0.6830 - val_acc: 0.8469\n",
            "Epoch 26/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5667 - acc: 0.8824 - val_loss: 0.7867 - val_acc: 0.8170\n",
            "Epoch 27/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6115 - acc: 0.8663 - val_loss: 1.0130 - val_acc: 0.7635\n",
            "Epoch 28/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.6561 - acc: 0.8516 - val_loss: 0.9837 - val_acc: 0.7663\n",
            "Epoch 29/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7086 - acc: 0.8371 - val_loss: 0.6715 - val_acc: 0.8495\n",
            "Epoch 30/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5308 - acc: 0.8944 - val_loss: 0.7468 - val_acc: 0.8200\n",
            "Epoch 31/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5795 - acc: 0.8762 - val_loss: 1.0543 - val_acc: 0.7474\n",
            "Epoch 32/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6256 - acc: 0.8613 - val_loss: 1.0025 - val_acc: 0.7589\n",
            "Epoch 33/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.7136 - acc: 0.8400 - val_loss: 0.6790 - val_acc: 0.8505\n",
            "Epoch 34/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5058 - acc: 0.9048 - val_loss: 0.8472 - val_acc: 0.7955\n",
            "Epoch 35/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5571 - acc: 0.8842 - val_loss: 0.8733 - val_acc: 0.7962\n",
            "Epoch 36/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6068 - acc: 0.8686 - val_loss: 0.9893 - val_acc: 0.7580\n",
            "Epoch 37/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6638 - acc: 0.8539 - val_loss: 0.6345 - val_acc: 0.8633\n",
            "Epoch 38/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4883 - acc: 0.9101 - val_loss: 0.8046 - val_acc: 0.8161\n",
            "Epoch 39/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5317 - acc: 0.8955 - val_loss: 0.8525 - val_acc: 0.7906\n",
            "Epoch 40/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5871 - acc: 0.8776 - val_loss: 0.8629 - val_acc: 0.7863\n",
            "Epoch 41/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6546 - acc: 0.8594 - val_loss: 0.6196 - val_acc: 0.8714\n",
            "Epoch 42/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4689 - acc: 0.9181 - val_loss: 0.8862 - val_acc: 0.7868\n",
            "Epoch 43/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5116 - acc: 0.9014 - val_loss: 0.9284 - val_acc: 0.7777\n",
            "Epoch 44/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5770 - acc: 0.8816 - val_loss: 0.8936 - val_acc: 0.7847\n",
            "Epoch 45/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6053 - acc: 0.8765 - val_loss: 0.6476 - val_acc: 0.8612\n",
            "Epoch 46/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4544 - acc: 0.9241 - val_loss: 0.6822 - val_acc: 0.8521\n",
            "Epoch 47/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4976 - acc: 0.9087 - val_loss: 1.0537 - val_acc: 0.7426\n",
            "Epoch 48/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5623 - acc: 0.8880 - val_loss: 0.9135 - val_acc: 0.7822\n",
            "Epoch 49/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6395 - acc: 0.8682 - val_loss: 0.6199 - val_acc: 0.8734\n",
            "Epoch 50/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4438 - acc: 0.9306 - val_loss: 0.6547 - val_acc: 0.8596\n",
            "Epoch 51/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4915 - acc: 0.9111 - val_loss: 0.8705 - val_acc: 0.8099\n",
            "Epoch 52/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5517 - acc: 0.8915 - val_loss: 1.1269 - val_acc: 0.7459\n",
            "Epoch 53/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5901 - acc: 0.8818 - val_loss: 0.6032 - val_acc: 0.8819\n",
            "Epoch 54/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4412 - acc: 0.9319 - val_loss: 0.6679 - val_acc: 0.8571\n",
            "Epoch 55/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4829 - acc: 0.9148 - val_loss: 1.0527 - val_acc: 0.7772\n",
            "Epoch 56/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5438 - acc: 0.8950 - val_loss: 1.4150 - val_acc: 0.6843\n",
            "Epoch 57/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.6041 - acc: 0.8815 - val_loss: 0.5944 - val_acc: 0.8861\n",
            "Epoch 58/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4318 - acc: 0.9375 - val_loss: 0.6859 - val_acc: 0.8583\n",
            "Epoch 59/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4681 - acc: 0.9214 - val_loss: 0.8005 - val_acc: 0.8309\n",
            "Epoch 60/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5454 - acc: 0.8959 - val_loss: 1.1522 - val_acc: 0.7262\n",
            "Epoch 61/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5894 - acc: 0.8894 - val_loss: 0.5834 - val_acc: 0.8903\n",
            "Epoch 62/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4283 - acc: 0.9395 - val_loss: 0.7882 - val_acc: 0.8255\n",
            "Epoch 63/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4622 - acc: 0.9245 - val_loss: 1.0082 - val_acc: 0.7959\n",
            "Epoch 64/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5346 - acc: 0.9020 - val_loss: 0.9285 - val_acc: 0.7929\n",
            "Epoch 65/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5849 - acc: 0.8890 - val_loss: 0.6061 - val_acc: 0.8837\n",
            "Epoch 66/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4229 - acc: 0.9421 - val_loss: 0.6701 - val_acc: 0.8653\n",
            "Epoch 67/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4539 - acc: 0.9279 - val_loss: 0.8720 - val_acc: 0.8095\n",
            "Epoch 68/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5291 - acc: 0.9035 - val_loss: 1.0464 - val_acc: 0.7616\n",
            "Epoch 69/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5616 - acc: 0.8979 - val_loss: 0.5863 - val_acc: 0.8891\n",
            "Epoch 70/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4180 - acc: 0.9454 - val_loss: 0.6652 - val_acc: 0.8695\n",
            "Epoch 71/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4468 - acc: 0.9307 - val_loss: 1.0681 - val_acc: 0.7650\n",
            "Epoch 72/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5248 - acc: 0.9050 - val_loss: 1.3999 - val_acc: 0.6876\n",
            "Epoch 73/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5711 - acc: 0.8949 - val_loss: 0.6009 - val_acc: 0.8849\n",
            "Epoch 74/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4125 - acc: 0.9474 - val_loss: 0.6681 - val_acc: 0.8631\n",
            "Epoch 75/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4410 - acc: 0.9335 - val_loss: 0.8398 - val_acc: 0.8278\n",
            "Epoch 76/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5163 - acc: 0.9094 - val_loss: 0.9076 - val_acc: 0.8040\n",
            "Epoch 77/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5535 - acc: 0.9012 - val_loss: 0.5755 - val_acc: 0.8949\n",
            "Epoch 78/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4063 - acc: 0.9485 - val_loss: 0.6941 - val_acc: 0.8629\n",
            "Epoch 79/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4396 - acc: 0.9339 - val_loss: 0.9438 - val_acc: 0.7920\n",
            "Epoch 80/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5133 - acc: 0.9106 - val_loss: 0.8837 - val_acc: 0.8057\n",
            "Epoch 81/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5766 - acc: 0.8947 - val_loss: 0.5963 - val_acc: 0.8880\n",
            "Epoch 82/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4045 - acc: 0.9520 - val_loss: 0.6250 - val_acc: 0.8825\n",
            "Epoch 83/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4403 - acc: 0.9343 - val_loss: 0.9210 - val_acc: 0.7989\n",
            "Epoch 84/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5021 - acc: 0.9161 - val_loss: 1.1384 - val_acc: 0.7454\n",
            "Epoch 85/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5547 - acc: 0.9014 - val_loss: 0.5813 - val_acc: 0.8929\n",
            "Epoch 86/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4110 - acc: 0.9482 - val_loss: 0.6581 - val_acc: 0.8716\n",
            "Epoch 87/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4309 - acc: 0.9381 - val_loss: 0.8190 - val_acc: 0.8283\n",
            "Epoch 88/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5093 - acc: 0.9133 - val_loss: 0.9469 - val_acc: 0.7864\n",
            "Epoch 89/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5773 - acc: 0.8951 - val_loss: 0.5966 - val_acc: 0.8920\n",
            "Epoch 90/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4039 - acc: 0.9527 - val_loss: 0.6265 - val_acc: 0.8778\n",
            "Epoch 91/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4282 - acc: 0.9402 - val_loss: 0.8912 - val_acc: 0.8175\n",
            "Epoch 92/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4986 - acc: 0.9165 - val_loss: 1.0038 - val_acc: 0.7692\n",
            "Epoch 93/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5792 - acc: 0.8982 - val_loss: 0.5814 - val_acc: 0.8962\n",
            "Epoch 94/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3978 - acc: 0.9552 - val_loss: 0.6302 - val_acc: 0.8777\n",
            "Epoch 95/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4247 - acc: 0.9421 - val_loss: 0.9642 - val_acc: 0.7994\n",
            "Epoch 96/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5082 - acc: 0.9154 - val_loss: 0.9613 - val_acc: 0.8029\n",
            "Epoch 97/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5359 - acc: 0.9098 - val_loss: 0.5782 - val_acc: 0.8969\n",
            "Epoch 98/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4051 - acc: 0.9533 - val_loss: 0.5948 - val_acc: 0.8890\n",
            "Epoch 99/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.4262 - acc: 0.9423 - val_loss: 0.8600 - val_acc: 0.8145\n",
            "Epoch 100/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5039 - acc: 0.9169 - val_loss: 0.9027 - val_acc: 0.8115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f81bba19fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtEWFolnvcXx",
        "colab_type": "code",
        "outputId": "f3aa5e2b-12b2-4967-aa08-85ba9c189207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Max Acc after 100 epochs : \",max(model.history.history['val_acc']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Acc after 100 epochs :  0.8969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sffmAByVsgMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"fi100.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0AFKBAxsjhH",
        "colab_type": "code",
        "outputId": "3b4c42d7-45a9-4675-ef18-96c1fa083925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#continue training for another 100 epochs after saving the model weights\n",
        "model.fit_generator(other_augs(datagen.flow(x_train, y_train, batch_size=batch_size)),\n",
        "                    steps_per_epoch = len(x_train) / 128, epochs=epochs, validation_data=other_augs(datagen.flow(x_test, y_test,batch_size=batch_size)),validation_steps=len(x_test)/128,callbacks=[clr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5368 - acc: 0.9115 - val_loss: 0.6238 - val_acc: 0.8841\n",
            "Epoch 2/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4069 - acc: 0.9550 - val_loss: 0.6231 - val_acc: 0.8825\n",
            "Epoch 3/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4243 - acc: 0.9439 - val_loss: 0.9090 - val_acc: 0.7961\n",
            "Epoch 4/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5069 - acc: 0.9170 - val_loss: 0.9239 - val_acc: 0.7998\n",
            "Epoch 5/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5170 - acc: 0.9179 - val_loss: 0.8531 - val_acc: 0.8085\n",
            "Epoch 6/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4561 - acc: 0.9391 - val_loss: 0.6475 - val_acc: 0.8805\n",
            "Epoch 7/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3882 - acc: 0.9612 - val_loss: 0.5361 - val_acc: 0.9098\n",
            "Epoch 8/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3521 - acc: 0.9729 - val_loss: 0.5241 - val_acc: 0.9163\n",
            "Epoch 9/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3451 - acc: 0.9752 - val_loss: 0.5273 - val_acc: 0.9129\n",
            "Epoch 10/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3502 - acc: 0.9702 - val_loss: 0.6345 - val_acc: 0.8775\n",
            "Epoch 11/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4117 - acc: 0.9446 - val_loss: 0.9951 - val_acc: 0.7850\n",
            "Epoch 12/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5082 - acc: 0.9160 - val_loss: 1.0559 - val_acc: 0.7684\n",
            "Epoch 13/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5114 - acc: 0.9206 - val_loss: 0.7956 - val_acc: 0.8379\n",
            "Epoch 14/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4600 - acc: 0.9396 - val_loss: 0.6676 - val_acc: 0.8749\n",
            "Epoch 15/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3862 - acc: 0.9616 - val_loss: 0.5501 - val_acc: 0.9059\n",
            "Epoch 16/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3524 - acc: 0.9721 - val_loss: 0.5195 - val_acc: 0.9165\n",
            "Epoch 17/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3472 - acc: 0.9742 - val_loss: 0.5316 - val_acc: 0.9110\n",
            "Epoch 18/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3501 - acc: 0.9696 - val_loss: 0.5969 - val_acc: 0.8888\n",
            "Epoch 19/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4148 - acc: 0.9423 - val_loss: 1.0141 - val_acc: 0.7736\n",
            "Epoch 20/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5095 - acc: 0.9154 - val_loss: 0.9716 - val_acc: 0.7813\n",
            "Epoch 21/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5039 - acc: 0.9244 - val_loss: 0.8632 - val_acc: 0.8175\n",
            "Epoch 22/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4552 - acc: 0.9412 - val_loss: 0.6158 - val_acc: 0.8892\n",
            "Epoch 23/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3822 - acc: 0.9636 - val_loss: 0.5337 - val_acc: 0.9143\n",
            "Epoch 24/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3500 - acc: 0.9738 - val_loss: 0.5280 - val_acc: 0.9161\n",
            "Epoch 25/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3471 - acc: 0.9743 - val_loss: 0.5273 - val_acc: 0.9163\n",
            "Epoch 26/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3475 - acc: 0.9712 - val_loss: 0.5905 - val_acc: 0.8909\n",
            "Epoch 27/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4181 - acc: 0.9431 - val_loss: 0.8028 - val_acc: 0.8336\n",
            "Epoch 28/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4996 - acc: 0.9199 - val_loss: 1.0286 - val_acc: 0.7748\n",
            "Epoch 29/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5121 - acc: 0.9201 - val_loss: 0.8867 - val_acc: 0.8035\n",
            "Epoch 30/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4489 - acc: 0.9435 - val_loss: 0.6437 - val_acc: 0.8814\n",
            "Epoch 31/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3812 - acc: 0.9639 - val_loss: 0.5450 - val_acc: 0.9109\n",
            "Epoch 32/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3451 - acc: 0.9755 - val_loss: 0.5289 - val_acc: 0.9162\n",
            "Epoch 33/100\n",
            "391/390 [==============================] - 22s 55ms/step - loss: 0.3427 - acc: 0.9754 - val_loss: 0.5245 - val_acc: 0.9169\n",
            "Epoch 34/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3527 - acc: 0.9694 - val_loss: 0.7113 - val_acc: 0.8606\n",
            "Epoch 35/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4071 - acc: 0.9467 - val_loss: 0.9800 - val_acc: 0.7936\n",
            "Epoch 36/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5114 - acc: 0.9165 - val_loss: 1.2288 - val_acc: 0.7220\n",
            "Epoch 37/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5070 - acc: 0.9230 - val_loss: 0.7515 - val_acc: 0.8449\n",
            "Epoch 38/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4453 - acc: 0.9453 - val_loss: 0.6397 - val_acc: 0.8823\n",
            "Epoch 39/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.3772 - acc: 0.9668 - val_loss: 0.5332 - val_acc: 0.9104\n",
            "Epoch 40/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.3477 - acc: 0.9762 - val_loss: 0.5257 - val_acc: 0.9170\n",
            "Epoch 41/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.3416 - acc: 0.9779 - val_loss: 0.5360 - val_acc: 0.9119\n",
            "Epoch 42/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3532 - acc: 0.9711 - val_loss: 0.6295 - val_acc: 0.8850\n",
            "Epoch 43/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4073 - acc: 0.9490 - val_loss: 0.8232 - val_acc: 0.8387\n",
            "Epoch 44/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5059 - acc: 0.9180 - val_loss: 1.0024 - val_acc: 0.7753\n",
            "Epoch 45/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5016 - acc: 0.9266 - val_loss: 0.6940 - val_acc: 0.8661\n",
            "Epoch 46/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4399 - acc: 0.9473 - val_loss: 0.6601 - val_acc: 0.8793\n",
            "Epoch 47/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3737 - acc: 0.9672 - val_loss: 0.5440 - val_acc: 0.9106\n",
            "Epoch 48/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3444 - acc: 0.9768 - val_loss: 0.5332 - val_acc: 0.9151\n",
            "Epoch 49/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3420 - acc: 0.9775 - val_loss: 0.5448 - val_acc: 0.9121\n",
            "Epoch 50/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3507 - acc: 0.9713 - val_loss: 0.6392 - val_acc: 0.8806\n",
            "Epoch 51/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4077 - acc: 0.9480 - val_loss: 0.8589 - val_acc: 0.8216\n",
            "Epoch 52/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5068 - acc: 0.9185 - val_loss: 0.8848 - val_acc: 0.8128\n",
            "Epoch 53/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5078 - acc: 0.9256 - val_loss: 0.8164 - val_acc: 0.8313\n",
            "Epoch 54/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4490 - acc: 0.9462 - val_loss: 0.6026 - val_acc: 0.8933\n",
            "Epoch 55/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3814 - acc: 0.9664 - val_loss: 0.5432 - val_acc: 0.9125\n",
            "Epoch 56/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3503 - acc: 0.9768 - val_loss: 0.5342 - val_acc: 0.9162\n",
            "Epoch 57/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3451 - acc: 0.9769 - val_loss: 0.5252 - val_acc: 0.9180\n",
            "Epoch 58/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3514 - acc: 0.9724 - val_loss: 0.6571 - val_acc: 0.8805\n",
            "Epoch 59/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4157 - acc: 0.9462 - val_loss: 1.0184 - val_acc: 0.7881\n",
            "Epoch 60/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5083 - acc: 0.9206 - val_loss: 1.0499 - val_acc: 0.7651\n",
            "Epoch 61/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5034 - acc: 0.9284 - val_loss: 0.7586 - val_acc: 0.8522\n",
            "Epoch 62/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4367 - acc: 0.9498 - val_loss: 0.6266 - val_acc: 0.8906\n",
            "Epoch 63/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3750 - acc: 0.9688 - val_loss: 0.5490 - val_acc: 0.9121\n",
            "Epoch 64/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3474 - acc: 0.9773 - val_loss: 0.5242 - val_acc: 0.9197\n",
            "Epoch 65/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3441 - acc: 0.9780 - val_loss: 0.5301 - val_acc: 0.9139\n",
            "Epoch 66/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3507 - acc: 0.9717 - val_loss: 0.6271 - val_acc: 0.8851\n",
            "Epoch 67/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4093 - acc: 0.9477 - val_loss: 1.2391 - val_acc: 0.7535\n",
            "Epoch 68/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5099 - acc: 0.9202 - val_loss: 1.2338 - val_acc: 0.7379\n",
            "Epoch 69/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5107 - acc: 0.9248 - val_loss: 0.7145 - val_acc: 0.8629\n",
            "Epoch 70/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4433 - acc: 0.9488 - val_loss: 0.6355 - val_acc: 0.8876\n",
            "Epoch 71/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3783 - acc: 0.9684 - val_loss: 0.5368 - val_acc: 0.9154\n",
            "Epoch 72/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3493 - acc: 0.9777 - val_loss: 0.5260 - val_acc: 0.9191\n",
            "Epoch 73/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3456 - acc: 0.9792 - val_loss: 0.5400 - val_acc: 0.9172\n",
            "Epoch 74/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3553 - acc: 0.9716 - val_loss: 0.6445 - val_acc: 0.8837\n",
            "Epoch 75/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4143 - acc: 0.9475 - val_loss: 0.8710 - val_acc: 0.8114\n",
            "Epoch 76/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5117 - acc: 0.9210 - val_loss: 0.8743 - val_acc: 0.8155\n",
            "Epoch 77/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4988 - acc: 0.9302 - val_loss: 0.7134 - val_acc: 0.8605\n",
            "Epoch 78/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4371 - acc: 0.9493 - val_loss: 0.5900 - val_acc: 0.9005\n",
            "Epoch 79/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3738 - acc: 0.9697 - val_loss: 0.5293 - val_acc: 0.9150\n",
            "Epoch 80/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3458 - acc: 0.9789 - val_loss: 0.5225 - val_acc: 0.9195\n",
            "Epoch 81/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3429 - acc: 0.9800 - val_loss: 0.5307 - val_acc: 0.9166\n",
            "Epoch 82/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3490 - acc: 0.9731 - val_loss: 0.6467 - val_acc: 0.8825\n",
            "Epoch 83/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4184 - acc: 0.9456 - val_loss: 1.0491 - val_acc: 0.7706\n",
            "Epoch 84/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4983 - acc: 0.9234 - val_loss: 0.8254 - val_acc: 0.8239\n",
            "Epoch 85/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5082 - acc: 0.9265 - val_loss: 0.7856 - val_acc: 0.8448\n",
            "Epoch 86/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4362 - acc: 0.9514 - val_loss: 0.6146 - val_acc: 0.8945\n",
            "Epoch 87/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3727 - acc: 0.9707 - val_loss: 0.5306 - val_acc: 0.9195\n",
            "Epoch 88/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3442 - acc: 0.9799 - val_loss: 0.5224 - val_acc: 0.9221\n",
            "Epoch 89/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3430 - acc: 0.9787 - val_loss: 0.5119 - val_acc: 0.9217\n",
            "Epoch 90/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3476 - acc: 0.9736 - val_loss: 0.7629 - val_acc: 0.8556\n",
            "Epoch 91/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.4150 - acc: 0.9481 - val_loss: 1.0163 - val_acc: 0.7848\n",
            "Epoch 92/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5164 - acc: 0.9187 - val_loss: 1.0531 - val_acc: 0.7756\n",
            "Epoch 93/100\n",
            "391/390 [==============================] - 22s 57ms/step - loss: 0.5019 - acc: 0.9294 - val_loss: 0.7912 - val_acc: 0.8392\n",
            "Epoch 94/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4412 - acc: 0.9502 - val_loss: 0.6363 - val_acc: 0.8878\n",
            "Epoch 95/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3767 - acc: 0.9715 - val_loss: 0.5366 - val_acc: 0.9164\n",
            "Epoch 96/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3512 - acc: 0.9788 - val_loss: 0.5266 - val_acc: 0.9180\n",
            "Epoch 97/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3478 - acc: 0.9785 - val_loss: 0.5336 - val_acc: 0.9171\n",
            "Epoch 98/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.3502 - acc: 0.9748 - val_loss: 0.6143 - val_acc: 0.8893\n",
            "Epoch 99/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.4202 - acc: 0.9471 - val_loss: 0.9594 - val_acc: 0.8127\n",
            "Epoch 100/100\n",
            "391/390 [==============================] - 22s 56ms/step - loss: 0.5143 - acc: 0.9208 - val_loss: 0.8827 - val_acc: 0.8004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f81a8667780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGlLvv2SFRoR",
        "colab_type": "code",
        "outputId": "43d0897e-b9e5-4192-ce65-06d571b2114b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Max Acc after 200 epochs : \",max(model.history.history['val_acc']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Acc after 200 epochs :  0.9221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i4BP0I4w7pk",
        "colab_type": "text"
      },
      "source": [
        "##  The following worked after trying multiple strategies before.\n",
        "1. SDG Learning rate schedule\n",
        "2. Gaussian blue : 15% of the images randomly\n",
        "3. Horizontal flip : 50% of the images randomly\n",
        "4. Weight decay : 5e-4\n",
        "5. Batch size : 128\n",
        "6. Random crop and padding : 20% of the images\n",
        "7. scaling(80%-120%), rotation(+-10), shear and translation on both x&y axis\n",
        "\n",
        "\n",
        "saving model after every 100 epochs\n",
        "\n",
        "## FINAL VAL ACC: 0.9221"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08mFSJXh06BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"final_92_valacc_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzQ0FPqEG0A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"final_92_valacc_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQe0J9Y5G5On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}