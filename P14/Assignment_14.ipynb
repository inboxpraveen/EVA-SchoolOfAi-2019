{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
    "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
    "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
    "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
    "EPOCHS =  20#@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
    "  fan = np.prod(shape[:-1])\n",
    "  bound = 1 / math.sqrt(fan)\n",
    "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN(tf.keras.Model):\n",
    "  def __init__(self, c_out):\n",
    "    super().__init__()\n",
    "    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=init_pytorch, use_bias=False)\n",
    "    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.nn.relu(self.bn(self.conv(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlk(tf.keras.Model):\n",
    "  def __init__(self, c_out, pool, res = False):\n",
    "    super().__init__()\n",
    "    self.conv_bn = ConvBN(c_out)\n",
    "    self.pool = pool\n",
    "    self.res = res\n",
    "    if self.res:\n",
    "      self.res1 = ConvBN(c_out)\n",
    "      self.res2 = ConvBN(c_out)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    h = self.pool(self.conv_bn(inputs))\n",
    "    if self.res:\n",
    "      h = h + self.res2(self.res1(h))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DavidNet(tf.keras.Model):\n",
    "  def __init__(self, c=64, weight=0.125):\n",
    "    super().__init__()\n",
    "    pool = tf.keras.layers.MaxPooling2D()\n",
    "    self.init_conv_bn = ConvBN(c)\n",
    "    self.blk1 = ResBlk(c*2, pool, res = True)\n",
    "    self.blk2 = ResBlk(c*4, pool)\n",
    "    self.blk3 = ResBlk(c*8, pool, res = True)\n",
    "    self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
    "    self.linear = tf.keras.layers.Dense(10, kernel_initializer=init_pytorch, use_bias=False)\n",
    "    self.weight = weight\n",
    "\n",
    "  def call(self, x, y):\n",
    "    h = self.pool(self.blk3(self.blk2(self.blk1(self.init_conv_bn(x)))))\n",
    "    h = self.linear(h) * self.weight\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
    "    loss = tf.reduce_sum(ce)\n",
    "    correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float32))\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.datasets.cifar import load_batch\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "# @keras_export('keras.datasets.cifar10.load_data')\n",
    "def load_data():\n",
    "  \"\"\"Loads CIFAR10 dataset.\n",
    "  Returns:\n",
    "      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "  \"\"\"\n",
    "  dirname = 'cifar-10-batches-py'\n",
    "  origin = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "  path = get_file(dirname, origin=origin, untar=True)\n",
    "#   path = dirname #fix\n",
    "  num_train_samples = 50000\n",
    "\n",
    "  x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n",
    "  y_train = np.empty((num_train_samples,), dtype='uint8')\n",
    "\n",
    "  for i in range(1, 6):\n",
    "    fpath = os.path.join(path, 'data_batch_' + str(i))\n",
    "    (x_train[(i - 1) * 10000:i * 10000, :, :, :],\n",
    "     y_train[(i - 1) * 10000:i * 10000]) = load_batch(fpath)\n",
    "\n",
    "  fpath = os.path.join(path, 'test_batch')\n",
    "  x_test, y_test = load_batch(fpath)\n",
    "\n",
    "  y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "  y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "  if K.image_data_format() == 'channels_last':\n",
    "    x_train = x_train.transpose(0, 2, 3, 1)\n",
    "    x_test = x_test.transpose(0, 2, 3, 1)\n",
    "\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "len_train, len_test = len(x_train), len(x_test)\n",
    "y_train = y_train.astype('int64').reshape(len_train)\n",
    "y_test = y_test.astype('int64').reshape(len_test)\n",
    "\n",
    "# train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "# train_std = np.std(x_train, axis=(0,1,2))\n",
    "\n",
    "normalize = lambda x: ((x - [125.31, 122.95, 113.87]) / [62.99, 62.09, 66.70]).astype('float32') ## custom normalization values\n",
    "pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
    "\n",
    "x_train = normalize(pad4(x_train))\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 40, 40, 3), (50000,)), ((10000, 32, 32, 3), (10000,)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cutout\n",
    "p=0.5\n",
    "s_l=0.02\n",
    "s_h=0.4\n",
    "r_1=0.3\n",
    "r_2=1/0.3\n",
    "v_l=0\n",
    "v_h=1\n",
    "pixel_level=True\n",
    "def eraser(input_img):\n",
    "  img_h, img_w, img_c = input_img.shape\n",
    "  p_1 = np.random.rand()\n",
    "\n",
    "  if p_1 > p:\n",
    "      return input_img\n",
    "\n",
    "  while True:\n",
    "      s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "      r = np.random.uniform(r_1, r_2)\n",
    "      w = int(np.sqrt(s / r))\n",
    "      h = int(np.sqrt(s * r))\n",
    "      left = np.random.randint(0, img_w)\n",
    "      top = np.random.randint(0, img_h)\n",
    "\n",
    "      if left + w <= img_w and top + h <= img_h:\n",
    "          break\n",
    "\n",
    "  if pixel_level:\n",
    "      c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "  else:\n",
    "      c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "  input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "  return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DavidNet()\n",
    "batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "\n",
    "lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//4, EPOCHS], [0, LEARNING_RATE, 0])[0]   ### adjusted learning rate for 20 epochs\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
    "opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)\n",
    "data_aug = lambda x, y: (tf.image.random_flip_left_right(tf.random_crop(eraser(x), [32, 32, 3])), y) ## with cutout\n",
    "\n",
    "test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIXUP\n",
    "\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=512, alpha=0.35, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=MixupGenerator(x_train,y_train))\n",
    "datagen.fit(x_train) ## fitting mixup generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3ab5161524b1284455bff8c651ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 lr: 0.08 train loss: 1.5673791424560546 train acc: 0.43296 val loss: 1.1037547515869142 val acc: 0.5988 time: 25.25158452987671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8d05ecf9a64e40b52bea74bd885359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2 lr: 0.16 train loss: 0.8314814947509765 train acc: 0.703 val loss: 0.9692330718994141 val acc: 0.6746 time: 45.889484167099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67770dbaa0af4fe3a97a3eeee72c0c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 3 lr: 0.24 train loss: 0.6351113354492187 train acc: 0.77852 val loss: 0.769729443359375 val acc: 0.7456 time: 66.39383506774902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db09cd1a69c744519ae1a0d5707d29df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4 lr: 0.32 train loss: 0.5458097592163086 train acc: 0.81184 val loss: 0.8992500427246094 val acc: 0.7248 time: 86.9902753829956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652cf81782d2474492357af89e01fc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 5 lr: 0.4 train loss: 0.47441636596679687 train acc: 0.83776 val loss: 0.8303089157104492 val acc: 0.7446 time: 108.11946868896484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc11045fcd8b40a190d1d9b92e9236f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 6 lr: 0.37333333333333335 train loss: 0.39200757537841796 train acc: 0.86554 val loss: 0.4611866455078125 val acc: 0.8464 time: 128.452486038208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39a19f07f714300af8982a5ba15fb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 7 lr: 0.3466666666666667 train loss: 0.3127730975341797 train acc: 0.89156 val loss: 0.36985001754760743 val acc: 0.8766 time: 148.90296411514282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb6000a6216436c9f04e1b3156b7b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 8 lr: 0.32 train loss: 0.2635659066772461 train acc: 0.90778 val loss: 0.37159037322998045 val acc: 0.875 time: 169.6071376800537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b46f8a0e60470fa739969d0977ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 9 lr: 0.29333333333333333 train loss: 0.2252009294128418 train acc: 0.92164 val loss: 0.3725972038269043 val acc: 0.8775 time: 190.02497100830078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2af4fb4378b4d3fbcaa01e209e42754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 10 lr: 0.2666666666666667 train loss: 0.19296568939208986 train acc: 0.93436 val loss: 0.33903170318603515 val acc: 0.8866 time: 210.64727449417114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f10c3be53074b26aefea1e316f04307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 11 lr: 0.24000000000000002 train loss: 0.1690389906311035 train acc: 0.9424 val loss: 0.2967694290161133 val acc: 0.9014 time: 231.1677188873291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c7be540e64421a943eb9ebb0ff06bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 12 lr: 0.21333333333333335 train loss: 0.1428063077545166 train acc: 0.95098 val loss: 0.28631438865661624 val acc: 0.912 time: 251.88637137413025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f3201920544cb2a5677e0f42a8519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 13 lr: 0.18666666666666668 train loss: 0.1242778050994873 train acc: 0.95786 val loss: 0.2549659557342529 val acc: 0.9204 time: 272.8673470020294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776b495df1624529bb8b793695fa8913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 14 lr: 0.16 train loss: 0.09915962455749512 train acc: 0.96634 val loss: 0.2740080863952637 val acc: 0.9181 time: 293.57465648651123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6158324869e242d19750c384bc77d567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 15 lr: 0.13333333333333336 train loss: 0.08518942386627197 train acc: 0.97222 val loss: 0.26593612747192386 val acc: 0.92 time: 314.8794147968292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45695719daa4e7c984fdd68765e8562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 16 lr: 0.10666666666666669 train loss: 0.07047460510253906 train acc: 0.97744 val loss: 0.25288733749389647 val acc: 0.9269 time: 335.0034348964691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a93faba07b4fed92726dbf17f71837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 17 lr: 0.08000000000000002 train loss: 0.057721946964263915 train acc: 0.9823 val loss: 0.24616375274658203 val acc: 0.932 time: 354.95211958885193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76551d2a48e44a88a47b4f59814df00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 18 lr: 0.053333333333333344 train loss: 0.04621327480316162 train acc: 0.98608 val loss: 0.23305194969177245 val acc: 0.9369 time: 375.40617775917053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994c5254c7524892b1165ee1c4adce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 19 lr: 0.026666666666666672 train loss: 0.03916914981842041 train acc: 0.9886 val loss: 0.22364716024398804 val acc: 0.9396 time: 395.8001956939697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b44addb09c406fbcb431fa5b22b106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 20 lr: 0.0 train loss: 0.03410941864013672 train acc: 0.9906 val loss: 0.2155842408180237 val acc: 0.941 time: 416.04638719558716\n"
     ]
    }
   ],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss = test_loss = train_acc = test_acc = 0.0\n",
    "  \n",
    "  tf.keras.backend.set_learning_phase(1)\n",
    "  for (x, y) in tqdm(train_set):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss, correct = model(x, y)\n",
    "\n",
    "    var = model.trainable_variables\n",
    "    grads = tape.gradient(loss, var)\n",
    "    for g, v in zip(grads, var):\n",
    "      g += v * WEIGHT_DECAY * BATCH_SIZE\n",
    "    opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
    "\n",
    "    train_loss += loss.numpy()\n",
    "    train_acc += correct.numpy()\n",
    "\n",
    "  tf.keras.backend.set_learning_phase(0)\n",
    "  for (x, y) in test_set:\n",
    "    loss, correct = model(x, y)\n",
    "    test_loss += loss.numpy()\n",
    "    test_acc += correct.numpy()\n",
    "    \n",
    "  print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
